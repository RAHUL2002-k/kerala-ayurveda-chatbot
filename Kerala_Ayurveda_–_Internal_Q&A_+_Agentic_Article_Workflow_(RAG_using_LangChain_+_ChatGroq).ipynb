{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAHUL2002-k/kerala-ayurveda-chatbot/blob/main/Kerala_Ayurveda_%E2%80%93_Internal_Q%26A_%2B_Agentic_Article_Workflow_(RAG_using_LangChain_%2B_ChatGroq).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvuWdq_U25aY"
      },
      "source": [
        "# Kerala Ayurveda – Internal Q&A + Agentic Article Workflow (RAG using LangChain + ChatGroq)\n",
        "\n",
        "This Colab notebook implements the assignment for an internal Kerala Ayurveda assistant using only the provided content pack.\n",
        "\n",
        "**Part A – Q&A RAG System**\n",
        "- Loads internal Ayurveda content (foundations, dosha guide, FAQs, products, treatments).\n",
        "- Chunks documents (with special handling for FAQs) and stores them in a FAISS vector store using HuggingFace embeddings.\n",
        "- Uses a similarity-based retriever to fetch the top-k chunks.\n",
        "- Calls a ChatGroq LLM to answer questions **only from the retrieved context**.\n",
        "- Returns both:\n",
        "  - A natural-language answer.\n",
        "  - Structured citations `{ doc_id, section_id }` for each supporting chunk.\n",
        "\n",
        "**Part B – Agentic Workflow with LangGraph**\n",
        "- Defines a small agentic pipeline:\n",
        "  1. Outline Agent – creates a sectioned article outline from a brief.\n",
        "  2. Writer Agent – generates a draft article using RAG and adds citation markers.\n",
        "  3. Fact-Checker Agent – checks grounding against the corpus and computes a grounding score.\n",
        "  4. Tone Editor Agent – applies style/tone rules based on the internal style guide.\n",
        "- Uses LangGraph to wire these agents into a simple step graph.\n",
        "\n",
        "**Tech Stack**\n",
        "- Python, Google Colab\n",
        "- `langchain`, `langchain-community`, `langchain-text-splitters`\n",
        "- `langchain-groq` for ChatGroq LLM calls\n",
        "- `langchain-huggingface` + `faiss-cpu` for vector embeddings and retrieval\n",
        "- `langgraph` for the agentic workflow (Part B)\n",
        "\n",
        "**How to run**\n",
        "1. Run the dependency installation cell.\n",
        "2. Set your `GROQ_API_KEY` in Colab secrets (`google.colab.userdata`).\n",
        "3. Run the data loading and chunking cells.\n",
        "4. Run the FAISS / retriever setup cell.\n",
        "5. Run the Q&A helper (`answer_user_query`) and the interactive loop cell to chat.\n",
        "6. Run the LangGraph cells to build and execute the agentic article workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI6IIIqspx7L"
      },
      "source": [
        "## 1. Install dependencies\n",
        "\n",
        "Install all required libraries for this notebook:\n",
        "\n",
        "- `langchain`, `langchain-community` and `langchain-text-splitters` for RAG building blocks\n",
        "- `langchain-groq` to call the ChatGroq LLM\n",
        "- `langchain-huggingface` and `faiss-cpu` for embeddings + vector store (FAISS)\n",
        "\n",
        "> Run this cell once at the start of the Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9tUE5IVY0t4"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  langchain-text-splitters \\\n",
        "  langchain-groq \\\n",
        "  langchain-huggingface \\\n",
        "  faiss-cpu \\\n",
        "  langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0IzvPTsthv-"
      },
      "source": [
        "## LLM & Core Library Initialization\n",
        "\n",
        "In this section, we:\n",
        "- Import all core LangChain, vector database, and embedding components\n",
        "- Configure the Large Language Model (LLM) using **ChatGroq**\n",
        "- Prepare the environment for both:\n",
        "  - Part A: RAG-based internal Q&A\n",
        "  - Part B: Agentic workflows using LangGraph\n",
        "\n",
        "The Groq API key is securely loaded from Colab secrets using `userdata`\n",
        "to avoid hardcoding sensitive credentials.\n",
        "\n",
        "The chosen LLM configuration prioritizes:\n",
        "- **Low temperature** for factual, grounded responses\n",
        "- **Moderate max tokens** to keep answers concise and cost-efficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiT_z9BGas9r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import TextLoader, CSVLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.documents import Document\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"your_grok_api_key_here\")\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",  # or another Groq-supported model you prefer\n",
        "    temperature=0.1,\n",
        "    max_tokens=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x2jqB5ItmsE"
      },
      "source": [
        "## Loading the Internal Ayurveda Corpus\n",
        "\n",
        "This section loads the complete internal Ayurveda knowledge corpus that the\n",
        "system is allowed to use.\n",
        "\n",
        "The corpus includes:\n",
        "- Foundational Ayurveda concepts\n",
        "- Dosha-specific guidance (Vata, Pitta, Kapha)\n",
        "- Patient-facing FAQs\n",
        "- Internal product notes (Ashwagandha, Brahmi Tailam, Triphala)\n",
        "- Internal treatment documentation (Stress Support Program)\n",
        "- A structured product catalog (CSV)\n",
        "\n",
        "Each document is loaded with explicit metadata such as:\n",
        "- **doc_id** — a stable identifier for citation and traceability\n",
        "- **doc_type** — a coarse classification (guide, faq, product, treatment)\n",
        "\n",
        "This metadata is later used for:\n",
        "- Accurate citations in RAG outputs\n",
        "- Filtering or prioritizing results in retrieval\n",
        "- Debugging and evaluation during development\n",
        "\n",
        "The system strictly relies on this corpus and does not use any external\n",
        "Ayurveda knowledge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3wjSlxgbfzM"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/\"\n",
        "\n",
        "file_specs = [\n",
        "    (\"ayurveda_foundations.md\", \"foundations\"),\n",
        "    (\"content_style_and_tone_guide.md\", \"style_guide\"),\n",
        "    (\"dosha_guide_vata_pitta_kapha.md\", \"dosha_guide\"),\n",
        "    (\"faq_general_ayurveda_patients.md\", \"faq\"),\n",
        "    (\"product_ashwagandha_tablets_internal.md\", \"product_ashwagandha\"),\n",
        "    (\"product_brahmi_tailam_internal.md\", \"product_brahmi_tailam\"),\n",
        "    (\"product_triphala_capsules_internal.md\", \"product_triphala\"),\n",
        "    (\"treatment_stress_support_program.md\", \"treatment_stress\"),\n",
        "]\n",
        "\n",
        "docs: list[Document] = []\n",
        "\n",
        "# Loading text files\n",
        "for filename, doc_id in file_specs:\n",
        "    loader = TextLoader(os.path.join(base_path, filename), encoding=\"utf-8\")\n",
        "    loaded = loader.load()\n",
        "    # attaching metadata\n",
        "    for d in loaded:\n",
        "        d.metadata[\"doc_id\"] = doc_id\n",
        "        d.metadata[\"doc_type\"] = doc_id.split(\"_\", 1)[0] # rough type\n",
        "    docs.extend(loaded)\n",
        "\n",
        "# 2.2 Loading products CSV as one chunk per row\n",
        "csv_path = os.path.join(base_path, \"products_catalog.csv\")\n",
        "if os.path.exists(csv_path):\n",
        "    csv_loader = CSVLoader(\n",
        "        file_path=csv_path,\n",
        "        encoding=\"utf-8\",\n",
        "        csv_args={\"delimiter\": \",\"},\n",
        "    )\n",
        "    csv_docs = csv_loader.load()\n",
        "    for d in csv_docs:\n",
        "        d.metadata[\"doc_id\"] = \"products_catalog\"\n",
        "        d.metadata[\"doc_type\"] = \"product_row\"\n",
        "    docs.extend(csv_docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnKR1IQZt0C6"
      },
      "source": [
        "## Document Chunking Strategy\n",
        "\n",
        "In this step, the loaded corpus is split into smaller, semantically meaningful\n",
        "chunks suitable for retrieval.\n",
        "\n",
        "### Chunking approach\n",
        "\n",
        "- A **RecursiveCharacterTextSplitter** is used for long-form documents\n",
        "  such as guides, treatments, and product notes.\n",
        "- The splitter prioritizes:\n",
        "  - Section boundaries (`##`)\n",
        "  - Paragraph breaks\n",
        "  - Line breaks\n",
        "  - Whitespace (as a last fallback)\n",
        "\n",
        "**Configuration:**\n",
        "- Chunk size: ~800 characters  \n",
        "- Chunk overlap: ~120 characters  \n",
        "\n",
        "This balances:\n",
        "- Preserving enough context for accurate answers\n",
        "- Avoiding overly large chunks that reduce retrieval precision\n",
        "\n",
        "###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9CufFUNot0G",
        "outputId": "bacb61e8-0083-4452-d46a-62d2dbfd5170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 35\n"
          ]
        }
      ],
      "source": [
        "splitter_long = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=120,\n",
        "    separators=[\"\\n##\", \"\\n\\n\", \"\\n\", \" \"],\n",
        ")\n",
        "\n",
        "split_docs: list[Document] = []\n",
        "\n",
        "for d in docs:\n",
        "    # special handling for FAQ: ensure one Q&A per chunk if possible\n",
        "    if d.metadata.get(\"doc_id\") == \"faq\":\n",
        "        # naive split by \"Q\" markers; adjust to actual format if needed\n",
        "        chunks = d.page_content.split(\"\\n\\nQ\")\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            text = (\"Q\" + chunk) if idx > 0 else chunk\n",
        "            split_docs.append(\n",
        "                Document(\n",
        "                    page_content=text.strip(),\n",
        "                    metadata={\n",
        "                        **d.metadata,\n",
        "                        \"section_id\": f\"qa_{idx}\",\n",
        "                    },\n",
        "                )\n",
        "            )\n",
        "    else:\n",
        "        sub_docs = splitter_long.split_documents([d])\n",
        "        for idx, sd in enumerate(sub_docs):\n",
        "            # keeping original metadata and adding section_id\n",
        "            sd.metadata[\"section_id\"] = sd.metadata.get(\"section_id\", f\"sec_{idx}\")\n",
        "            split_docs.append(sd)\n",
        "\n",
        "print(f\"Total chunks: {len(split_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU1pVVOLt9lQ"
      },
      "source": [
        "## Embeddings & Vector Store Construction\n",
        "\n",
        "This section converts the processed document chunks into vector embeddings\n",
        "and stores them in a vector database for semantic retrieval.\n",
        "\n",
        "### Embeddings\n",
        "\n",
        "- We use **HuggingFace sentence embeddings**\n",
        "  (`sentence-transformers/all-MiniLM-L6-v2`)\n",
        "- This model provides:\n",
        "  - Strong performance on short-to-medium length text\n",
        "  - Good semantic similarity for question–answer tasks\n",
        "  - Low latency and reasonable memory usage for Colab\n",
        "\n",
        "These properties make it well-suited for an internal RAG system where:\n",
        "- The corpus siz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ENarMWOc4p-"
      },
      "outputs": [],
      "source": [
        "# Embeddings and storing in faiss\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 8},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPK_TRBVuybV"
      },
      "source": [
        "### 6. Define system prompt and `answer_user_query` function\n",
        "\n",
        "This cell defines the core Q&A behavior:\n",
        "\n",
        "1. **SYSTEM_PROMPT**\n",
        "   - Instructs the LLM to:\n",
        "     - Act as an internal Kerala Ayurveda assistant.\n",
        "     - Use **only** the provided context.\n",
        "     - Say “I don't know” when the answer is not in the corpus.\n",
        "   - Enforces safety and tone:\n",
        "     - Clear, warm, reassuring.\n",
        "     - No absolute guarantees.\n",
        "     - Emphasize consulting qualified Ayurveda physicians when needed.\n",
        "   - Asks the model to reference context blocks with `[1]`, `[2]`, etc.\n",
        "\n",
        "2. **`build_context_and_citation_map(docs)`**\n",
        "   - Takes retrieved chunks and:\n",
        "     - Builds a numbered context string like:\n",
        "       - `[1] doc_id=product_ashwagandha, section_id=sec_0`\n",
        "     - Keeps a mapping of:\n",
        "       - `index` → `doc_id`, `section_id`\n",
        "   - This is used both in the prompt and in the final citations.\n",
        "\n",
        "3. **`answer_user_query(query: str)`**\n",
        "   - Full RAG pipeline:\n",
        "     - Uses `retriever.invoke(query)` to fetch relevant chunks.\n",
        "     - Builds context + citation map.\n",
        "     - Formats the system prompt with `context` and `question`.\n",
        "     - Calls the Groq LLM (`llm.invoke(prompt)`).\n",
        "   - Returns:\n",
        "     - `\"answer\"`: model’s natural language answer.\n",
        "     - `\"citations\"`: list of `{doc_id, section_id}` for transparency and debugging.\n",
        "\n",
        "This is the function you would expose to the internal Q&A front-end or API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfH3q5GOdGFP"
      },
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "SYSTEM_PROMPT = dedent(\"\"\"\n",
        "You are an internal Q&A assistant for Kerala Ayurveda.\n",
        "Use ONLY the provided context to answer questions.\n",
        "If the answer is not in the context, say you don't know and suggest consulting a qualified Ayurveda physician.\n",
        "\n",
        "Tone guidelines (from internal style guide):\n",
        "- Be clear, warm, and reassuring.\n",
        "- Avoid absolute claims or guarantees.\n",
        "- Emphasize safety and personalized consultation when appropriate.\n",
        "\n",
        "When you use a fact from a specific context block, reference it with [1], [2], etc.,\n",
        "where the number refers to the block index.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (with inline citation markers like [1], [2] where relevant):\n",
        "\"\"\").strip()\n",
        "\n",
        "\n",
        "def build_context_and_citation_map(docs: list[Document]):\n",
        "    \"\"\"Build context string with numbered blocks and return citation metadata.\"\"\"\n",
        "    blocks = []\n",
        "    citations = []\n",
        "    for i, d in enumerate(docs, start=1):\n",
        "        doc_id = d.metadata.get(\"doc_id\", \"unknown_doc\")\n",
        "        section_id = d.metadata.get(\"section_id\", str(i))\n",
        "        header = f\"[{i}] doc_id={doc_id}, section_id={section_id}\"\n",
        "        blocks.append(f\"{header}\\n{d.page_content}\\n\")\n",
        "        citations.append(\n",
        "            {\n",
        "                \"index\": i,\n",
        "                \"doc_id\": doc_id,\n",
        "                \"section_id\": section_id,\n",
        "            }\n",
        "        )\n",
        "    context = \"\\n\\n\".join(blocks)\n",
        "    return context, citations\n",
        "\n",
        "\n",
        "def answer_user_query(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Main function expected by the assignment.\n",
        "    Returns:\n",
        "    {\n",
        "        \"answer\": str,\n",
        "        \"citations\": [ { \"doc_id\": str, \"section_id\": str } ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    # 1. Retrieving relevant chunks\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "    # 2. Building context + citation map\n",
        "    context, citation_map = build_context_and_citation_map(retrieved_docs)\n",
        "\n",
        "    # 3. Build the prompt\n",
        "    prompt = SYSTEM_PROMPT.format(context=context, question=query)\n",
        "\n",
        "    # 4. Calling LLM (ChatGroq)\n",
        "    llm_response = llm.invoke(prompt)\n",
        "    if hasattr(llm_response, \"content\"):\n",
        "        answer_text = llm_response.content\n",
        "    else:\n",
        "        answer_text = str(llm_response)\n",
        "\n",
        "    # we return ALL retrieved docs as citations.\n",
        "\n",
        "    citations = [\n",
        "        {\"doc_id\": c[\"doc_id\"], \"section_id\": c[\"section_id\"]}\n",
        "        for c in citation_map\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer_text,\n",
        "        \"citations\": citations,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzzZjUGvu238"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWRjyPvJzV"
      },
      "source": [
        "## Interactive Q&A Loop (Manual Testing)\n",
        "\n",
        "This cell provides a simple interactive interface for manually testing\n",
        "the RAG-based internal Q&A system.\n",
        "\n",
        "### What this does\n",
        "\n",
        "- Prompts the user to enter natural-language questions.\n",
        "- For each question:\n",
        "  1. Calls `answer_user_query(query)`.\n",
        "  2. Prints the generated answer.\n",
        "  3. Prints the associated citations:\n",
        "     - `doc_id` and `section_id` for each retrieved context chunk.\n",
        "- Continues running until the user types **`exit`**.\n",
        "\n",
        "### Why this is useful\n",
        "\n",
        "- Allows quick, iterative testing of retrieval quality and answer grounding.\n",
        "- Makes it easy to inspect:\n",
        "  - Whether answers are actually backed by internal documents.\n",
        "  - Which parts of the corpus the model relied on.\n",
        "- Serves as a lightweight stand-in for an internal UI or API.\n",
        "\n",
        "In a production setting, this loop could be replaced by:\n",
        "- A simple web UI (e.g., Gradio or Streamlit), or\n",
        "- An internal API endpoint exposed to other teams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfHS9g3CetB9",
        "outputId": "80cb08cd-1017-4d9d-be28-28ae89886b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER:\n",
            " Ashwagandha is traditionally used to support the body's ability to adapt to stress, promote calmness and emotional balance, support strength and stamina, and help maintain restful sleep [2]. It is considered an adaptogen, which means it may help your body resist stressors and promote overall well-being. Our Ashwagandha Stress Balance Tablets are designed to provide daily support for stress resilience and restful sleep, inspired by the traditional use of Ashwagandha in Ayurveda [2]. However, it's essential to note that individuals with certain health conditions, such as thyroid disorders or autoimmune conditions, or those taking long-term medication, should consult their healthcare provider before using Ashwagandha [1]. Additionally, pregnant individuals should only use Ashwagandha under the guidance of a qualified healthcare practitioner [1].\n",
            "\n",
            "CITATIONS: [{'doc_id': 'product_ashwagandha', 'section_id': 'sec_2'}, {'doc_id': 'product_ashwagandha', 'section_id': 'sec_0'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'product_triphala', 'section_id': 'sec_0'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'foundations', 'section_id': 'sec_2'}, {'doc_id': 'foundations', 'section_id': 'sec_3'}, {'doc_id': 'style_guide', 'section_id': 'sec_0'}]\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "ANSWER:\n",
            " I'm happy to help you with your question. However, I must emphasize that it's essential to prioritize your safety and the well-being of your baby during pregnancy. According to our safety guidelines [1], Ashwagandha is not recommended for pregnant individuals without personalized professional advice. \n",
            "\n",
            "I strongly advise you to consult your healthcare provider before taking any supplement, including Ashwagandha, during pregnancy. They can provide you with personalized guidance and help you make an informed decision about what's best for you and your baby. \n",
            "\n",
            "I don't know the exact dosage that would be safe for you, and I wouldn't want to provide any information that might be misleading. Instead, I recommend that you consult with your healthcare provider to discuss your options and determine the best course of action for your specific situation.\n",
            "\n",
            "CITATIONS: [{'doc_id': 'product_ashwagandha', 'section_id': 'sec_2'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'product_ashwagandha', 'section_id': 'sec_0'}, {'doc_id': 'foundations', 'section_id': 'sec_3'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'product_triphala', 'section_id': 'sec_2'}, {'doc_id': 'product_triphala', 'section_id': 'sec_0'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}]\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "ANSWER:\n",
            " I'm so sorry to hear that you're experiencing loose motion. While I'm not a substitute for a qualified Ayurveda physician, I can offer some general guidance. According to our understanding of Ayurvedic principles [7], digestive issues can be related to an imbalance of the doshas, particularly Pitta, which is associated with digestion and metabolism.\n",
            "\n",
            "For loose motion, you may want to consider consuming something that can help soothe and calm your digestive system. However, I don't have specific information on what to consume in this context. It's essential to consult a qualified Ayurveda physician who can provide personalized advice based on your unique constitution and health status.\n",
            "\n",
            "That being said, our Triphala Capsules [5] are a classical three-fruit blend that Ayurveda uses to support digestion and regularity. Many people find that, alongside mindful eating and adequate hydration, it helps them feel lighter and more comfortable after meals [1]. However, it's crucial to consult with a healthcare provider before using any new supplement, especially if you have any underlying health conditions or concerns [3].\n",
            "\n",
            "Please consult a qualified Ayurveda physician for personalized guidance on managing loose motion. They can help you determine the best course of action and recommend suitable remedies or lifestyle changes to support your digestive health.\n",
            "\n",
            "CITATIONS: [{'doc_id': 'style_guide', 'section_id': 'sec_3'}, {'doc_id': 'product_triphala', 'section_id': 'sec_1'}, {'doc_id': 'product_triphala', 'section_id': 'sec_2'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'product_triphala', 'section_id': 'sec_0'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}, {'doc_id': 'foundations', 'section_id': 'sec_1'}, {'doc_id': 'products_catalog', 'section_id': 'sec_0'}]\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    q = input(\"Ask a question about Kerala Ayurveda (or 'exit'): \")\n",
        "    if q.lower() == \"exit\":\n",
        "        break\n",
        "    res = answer_user_query(q)\n",
        "    print(\"\\nANSWER:\\n\", res[\"answer\"])\n",
        "    print(\"\\nCITATIONS:\", res[\"citations\"])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txIdzrMz1vbF"
      },
      "source": [
        "## 8. LangGraph agentic workflow for Ayurveda article generation (Part B)\n",
        "\n",
        "This cell sketches the **agentic pipeline** described in the assignment using LangGraph:\n",
        "\n",
        "### Shared state (`ArticleState`)\n",
        "A typed dictionary shared across agents, containing:\n",
        "- `brief`: the input article brief (topic, angle, target reader).\n",
        "- `outline`: structured outline generated by the Outline Agent.\n",
        "- `draft`: full article draft generated by the Writer Agent.\n",
        "- `citations`: list of `{ doc_id, section_id }` used in the draft.\n",
        "- `fact_check_report`: grounding / fact-check result.\n",
        "- `tone_issues`: any style/tone problems detected.\n",
        "- `final_draft`: post-edited draft ready for human review.\n",
        "- `status`: simple status flag for debugging.\n",
        "\n",
        "### Agents (nodes)\n",
        "- **`outline_agent`**\n",
        "  - Input: `brief`.\n",
        "  - Output: `outline` with sections + descriptions, sets `status = \"drafting\"`.\n",
        "- **`writer_agent`**\n",
        "  - For each outline section:\n",
        "    - (Conceptually) calls the RAG Q&A tools to fetch context.\n",
        "    - Uses LLM to generate section content with citation markers.\n",
        "  - Combines sections into `draft` and collects `citations`.\n",
        "  - Sets `status = \"fact_checking\"`.\n",
        "- **`fact_checker_agent`**\n",
        "  - (Conceptually) re-runs RAG to validate claims in the draft.\n",
        "  - Fills `fact_check_report` and an `overall_grounding_score`.\n",
        "  - Sets `status = \"editing\"`.\n",
        "- **`tone_editor_agent`**\n",
        "  - Applies style/tone adjustments based on the internal style guide.\n",
        "  - Writes `final_draft` and `tone_issues`, sets `status = \"done\"`.\n",
        "\n",
        "### Graph wiring\n",
        "- Entry node: `outline_agent`.\n",
        "- Linear path: `outline_agent → writer_agent → fact_checker_agent`.\n",
        "- Conditional edge after fact checking:\n",
        "  - If `overall_grounding_score < 0.75` → loop back to `writer_agent` for revision.\n",
        "  - Else → proceed to `tone_editor_agent`.\n",
        "- `tone_editor_agent → END`.\n",
        "\n",
        "A `MemorySaver` checkpointer is used so the workflow state can be inspected across steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFkg8H5QnSLQ"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Shared state type\n",
        "\n",
        "class ArticleState(TypedDict, total=False):\n",
        "    brief: Dict\n",
        "    outline: Dict\n",
        "    draft: str\n",
        "    citations: List[Dict[str, str]]\n",
        "    fact_check_report: Dict\n",
        "    tone_issues: List[str]\n",
        "    final_draft: str\n",
        "    status: str\n",
        "\n",
        "\n",
        "# Node functions (agents)\n",
        "\n",
        "def outline_agent(state: ArticleState) -> ArticleState:\n",
        "    brief = state[\"brief\"]\n",
        "    # Call LLM (ChatGroq) with simple prompt to create outline (OPTIONALLY call RAG)\n",
        "    # outline = ...\n",
        "    outline = {\n",
        "        \"sections\": [\n",
        "            {\"heading\": \"Introduction to Vata Dosha\", \"description\": \"Plain explanation...\"},\n",
        "            {\"heading\": \"Signs of Vata Imbalance\", \"description\": \"Common patterns...\"},\n",
        "            {\"heading\": \"Lifestyle and Dietary Support\", \"description\": \"High-level tips...\"},\n",
        "            {\"heading\": \"When to Consult a Physician\", \"description\": \"Safety guidance...\"},\n",
        "        ]\n",
        "    }\n",
        "    state[\"outline\"] = outline\n",
        "    state[\"status\"] = \"drafting\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def writer_agent(state: ArticleState) -> ArticleState:\n",
        "    # For each section in outline:\n",
        "    #   - call a RAG tool to get chunks\n",
        "    #   - call LLM to write that section with citation markers\n",
        "    # combine sections into `draft`\n",
        "    # collect citations from RAG calls\n",
        "    draft = \"# \" + state[\"brief\"][\"title\"] + \"\\n\\n...\"  # generated text\n",
        "    citations = [\n",
        "        {\"doc_id\": \"dosha_guide_vata_pitta_kapha.md\", \"section_id\": \"vata_overview_0\"}\n",
        "    ]\n",
        "    state[\"draft\"] = draft\n",
        "    state[\"citations\"] = citations\n",
        "    state[\"status\"] = \"fact_checking\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def fact_checker_agent(state: ArticleState) -> ArticleState:\n",
        "    # Use RAG again to check each claim in state[\"draft\"]\n",
        "    fact_report = {\n",
        "        \"unsupported_claims\": [],\n",
        "        \"missing_citations\": [],\n",
        "        \"overall_grounding_score\": 0.9,\n",
        "    }\n",
        "    state[\"fact_check_report\"] = fact_report\n",
        "    state[\"status\"] = \"editing\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def tone_editor_agent(state: ArticleState) -> ArticleState:\n",
        "    # Load style guide from corpus or env variable; call LLM to rewrite\n",
        "    final_draft = state[\"draft\"]  # in reality, call LLM with style prompt\n",
        "    tone_issues = []\n",
        "    state[\"final_draft\"] = final_draft\n",
        "    state[\"tone_issues\"] = tone_issues\n",
        "    state[\"status\"] = \"done\"\n",
        "    return state\n",
        "\n",
        "\n",
        "# Graph wiring\n",
        "\n",
        "builder = StateGraph(ArticleState)\n",
        "\n",
        "builder.add_node(\"outline_agent\", outline_agent)\n",
        "builder.add_node(\"writer_agent\", writer_agent)\n",
        "builder.add_node(\"fact_checker_agent\", fact_checker_agent)\n",
        "builder.add_node(\"tone_editor_agent\", tone_editor_agent)\n",
        "\n",
        "builder.set_entry_point(\"outline_agent\")\n",
        "\n",
        "builder.add_edge(\"outline_agent\", \"writer_agent\")\n",
        "builder.add_edge(\"writer_agent\", \"fact_checker_agent\")\n",
        "\n",
        "# If grounding too low, we can loop back to writer_agent\n",
        "def route_after_fact_check(state: ArticleState) -> str:\n",
        "    report = state.get(\"fact_check_report\", {})\n",
        "    score = report.get(\"overall_grounding_score\", 0.0)\n",
        "    if score < 0.75:\n",
        "        return \"writer_agent\"\n",
        "    return \"tone_editor_agent\"\n",
        "\n",
        "builder.add_conditional_edges(\"fact_checker_agent\", route_after_fact_check)\n",
        "\n",
        "builder.add_edge(\"tone_editor_agent\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "app = builder.compile(checkpointer=memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMgXSKZ1zmW"
      },
      "source": [
        "## Step 2 – Short Reflection\n",
        "\n",
        "- **Time spent:**  \n",
        "  I spent approximately **6–8 hours** overall understanding the content pack, designing the RAG pipeline, implementing the FAISS-based retriever, and sketching the agentic workflow using LangGraph.\n",
        "\n",
        "- **Most interesting part:**  \n",
        "  The most interesting aspect was designing the **fact-checking step** in the agentic workflow and thinking about how to systematically enforce grounding using the same RAG corpus instead of relying on model trust alone.\n",
        "\n",
        "- **Most unclear or challenging aspect:**  \n",
        "  One challenge was balancing **chunk size and retrieval depth** to ensure sufficient context without overwhelming the LLM or introducing noisy evidence, especially for multi-part or safety-related questions.\n",
        "\n",
        "- **Use of AI tools:**  \n",
        "  I used AI tools (including ChatGPT) to:\n",
        "  - Brainstorm RAG and agentic workflow designs.\n",
        "  - Draft and refine code scaffolding for LangChain and LangGraph.\n",
        "  - Debug Colab-specific dependency and import issues.\n",
        "  - Rephrase explanations and comments for clarity and conciseness.\n",
        "\n",
        "- **Final judgment:**  \n",
        "  The final solution prioritizes **clarity, grounding, and pragmatic design** over over-engineering, with clear extension points for future improvements such as stricter claim validation or UI integration.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMxswuv7QZZeiEWWsRmxv6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}